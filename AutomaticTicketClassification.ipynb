{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhR-ZUkwJrFn"
   },
   "source": [
    "# Automatic Ticket Classification Case Study\n",
    "    Aman Chourasiya\n",
    "<hr style=\"border:0.2px solid gray\"> </hr>\n",
    "\n",
    "We need to build a model that is able to classify customer complaints based on the products/services. By doing so, we can segregate these tickets into their relevant categories and, therefore, help in the quick resolution of the issue.\n",
    "\n",
    "We will be doing topic modelling on the <b>.json</b> data provided by the company. Since this data is not labelled, we need to apply NMF to analyse patterns and classify tickets into the following five clusters based on their products/services:\n",
    "\n",
    "* Credit card / Prepaid card\n",
    "\n",
    "* Bank account services\n",
    "\n",
    "* Theft/Dispute reporting\n",
    "\n",
    "* Mortgages/loans\n",
    "\n",
    "* OthersÂ \n",
    "\n",
    "\n",
    "With the help of topic modelling, we will be able to map each ticket onto its respective department/category. we can then use this data to train any supervised model such as logistic regression, decision tree or random forest. Using this trained model, we can classify any new customer complaint support ticket into its relevant department."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuLFIymAL58u"
   },
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T02:52:51.461586Z",
     "iopub.status.busy": "2022-03-23T02:52:51.461189Z",
     "iopub.status.idle": "2022-03-23T02:53:04.204965Z",
     "shell.execute_reply": "2022-03-23T02:53:04.203767Z",
     "shell.execute_reply.started": "2022-03-23T02:52:51.461475Z"
    },
    "id": "S4bOGDUtyKN8",
    "outputId": "ea13d6d1-8080-49a9-f45f-a82e42a8d56b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swifter in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (1.4.0)\n",
      "Requirement already satisfied: dask[dataframe]>=2.10.0 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from swifter) (2022.7.0)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from swifter) (1.4.4)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from swifter) (4.64.1)\n",
      "Requirement already satisfied: psutil>=5.6.6 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from swifter) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from dask[dataframe]>=2.10.0->swifter) (2022.7.1)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from dask[dataframe]>=2.10.0->swifter) (2.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from dask[dataframe]>=2.10.0->swifter) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from dask[dataframe]>=2.10.0->swifter) (21.3)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from dask[dataframe]>=2.10.0->swifter) (0.11.2)\n",
      "Requirement already satisfied: partd>=0.3.10 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from dask[dataframe]>=2.10.0->swifter) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.18 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from dask[dataframe]>=2.10.0->swifter) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.0->swifter) (2022.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->dask[dataframe]>=2.10.0->swifter) (3.0.9)\n",
      "Requirement already satisfied: locket in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from partd>=0.3.10->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.0.0->swifter) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from wordcloud) (1.21.5)\n",
      "Requirement already satisfied: matplotlib in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from wordcloud) (3.5.2)\n",
      "Requirement already satisfied: pillow in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /Users/amanchourasiya/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T02:53:04.207067Z",
     "iopub.status.busy": "2022-03-23T02:53:04.206840Z",
     "iopub.status.idle": "2022-03-23T02:53:18.079157Z",
     "shell.execute_reply": "2022-03-23T02:53:18.078374Z",
     "shell.execute_reply.started": "2022-03-23T02:53:04.207037Z"
    },
    "id": "O-Q9pqrcJrFr"
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, string\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load(disable=['parser','ner'])\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from collections import Counter\n",
    "import swifter\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from plotly.offline import plot\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtRLCsNVJrFt"
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "The data is in JSON format and we need to convert it to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T02:54:07.748433Z",
     "iopub.status.busy": "2022-03-23T02:54:07.748080Z",
     "iopub.status.idle": "2022-03-23T02:54:12.805145Z",
     "shell.execute_reply": "2022-03-23T02:54:12.804453Z",
     "shell.execute_reply.started": "2022-03-23T02:54:07.748398Z"
    },
    "id": "puVzIf_iJrFt"
   },
   "outputs": [],
   "source": [
    "# Opening JSON file \n",
    "f = open('complaints-2021-05-14_08_16.json')\n",
    " \n",
    "data = json.load(f)\n",
    "df=pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xYpH-sAJrFu"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T02:54:19.045862Z",
     "iopub.status.busy": "2022-03-23T02:54:19.045556Z",
     "iopub.status.idle": "2022-03-23T02:54:19.083151Z",
     "shell.execute_reply": "2022-03-23T02:54:19.082299Z",
     "shell.execute_reply.started": "2022-03-23T02:54:19.045829Z"
    },
    "id": "Lf8ufHH5JrFu",
    "outputId": "c49c9d02-e8b8-4a4d-8cbf-ccb444650c11"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_index</th>\n",
       "      <th>_type</th>\n",
       "      <th>_id</th>\n",
       "      <th>_score</th>\n",
       "      <th>_source.tags</th>\n",
       "      <th>_source.zip_code</th>\n",
       "      <th>_source.complaint_id</th>\n",
       "      <th>_source.issue</th>\n",
       "      <th>_source.date_received</th>\n",
       "      <th>_source.state</th>\n",
       "      <th>...</th>\n",
       "      <th>_source.company_response</th>\n",
       "      <th>_source.company</th>\n",
       "      <th>_source.submitted_via</th>\n",
       "      <th>_source.date_sent_to_company</th>\n",
       "      <th>_source.company_public_response</th>\n",
       "      <th>_source.sub_product</th>\n",
       "      <th>_source.timely</th>\n",
       "      <th>_source.complaint_what_happened</th>\n",
       "      <th>_source.sub_issue</th>\n",
       "      <th>_source.consumer_consent_provided</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>complaint-public-v2</td>\n",
       "      <td>complaint</td>\n",
       "      <td>3211475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>90301</td>\n",
       "      <td>3211475</td>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "      <td>2019-04-13T12:00:00-05:00</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>JPMORGAN CHASE &amp; CO.</td>\n",
       "      <td>Web</td>\n",
       "      <td>2019-04-13T12:00:00-05:00</td>\n",
       "      <td>None</td>\n",
       "      <td>Credit card debt</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>Debt is not yours</td>\n",
       "      <td>Consent not provided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>complaint-public-v2</td>\n",
       "      <td>complaint</td>\n",
       "      <td>3229299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Servicemember</td>\n",
       "      <td>319XX</td>\n",
       "      <td>3229299</td>\n",
       "      <td>Written notification about debt</td>\n",
       "      <td>2019-05-01T12:00:00-05:00</td>\n",
       "      <td>GA</td>\n",
       "      <td>...</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>JPMORGAN CHASE &amp; CO.</td>\n",
       "      <td>Web</td>\n",
       "      <td>2019-05-01T12:00:00-05:00</td>\n",
       "      <td>None</td>\n",
       "      <td>Credit card debt</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Good morning my name is XXXX XXXX and I apprec...</td>\n",
       "      <td>Didn't receive enough information to verify debt</td>\n",
       "      <td>Consent provided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>complaint-public-v2</td>\n",
       "      <td>complaint</td>\n",
       "      <td>3199379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>77069</td>\n",
       "      <td>3199379</td>\n",
       "      <td>Other features, terms, or problems</td>\n",
       "      <td>2019-04-02T12:00:00-05:00</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>JPMORGAN CHASE &amp; CO.</td>\n",
       "      <td>Web</td>\n",
       "      <td>2019-04-02T12:00:00-05:00</td>\n",
       "      <td>None</td>\n",
       "      <td>General-purpose credit card or charge card</td>\n",
       "      <td>Yes</td>\n",
       "      <td>I upgraded my XXXX XXXX card in XX/XX/2018 and...</td>\n",
       "      <td>Problem with rewards from credit card</td>\n",
       "      <td>Consent provided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>complaint-public-v2</td>\n",
       "      <td>complaint</td>\n",
       "      <td>2673060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>48066</td>\n",
       "      <td>2673060</td>\n",
       "      <td>Trouble during payment process</td>\n",
       "      <td>2017-09-13T12:00:00-05:00</td>\n",
       "      <td>MI</td>\n",
       "      <td>...</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>JPMORGAN CHASE &amp; CO.</td>\n",
       "      <td>Web</td>\n",
       "      <td>2017-09-14T12:00:00-05:00</td>\n",
       "      <td>None</td>\n",
       "      <td>Conventional home mortgage</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>Consent not provided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>complaint-public-v2</td>\n",
       "      <td>complaint</td>\n",
       "      <td>3203545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>10473</td>\n",
       "      <td>3203545</td>\n",
       "      <td>Fees or interest</td>\n",
       "      <td>2019-04-05T12:00:00-05:00</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>JPMORGAN CHASE &amp; CO.</td>\n",
       "      <td>Referral</td>\n",
       "      <td>2019-04-05T12:00:00-05:00</td>\n",
       "      <td>None</td>\n",
       "      <td>General-purpose credit card or charge card</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>Charged too much interest</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                _index      _type      _id  _score   _source.tags  \\\n",
       "0  complaint-public-v2  complaint  3211475     0.0           None   \n",
       "1  complaint-public-v2  complaint  3229299     0.0  Servicemember   \n",
       "2  complaint-public-v2  complaint  3199379     0.0           None   \n",
       "3  complaint-public-v2  complaint  2673060     0.0           None   \n",
       "4  complaint-public-v2  complaint  3203545     0.0           None   \n",
       "\n",
       "  _source.zip_code _source.complaint_id                       _source.issue  \\\n",
       "0            90301              3211475   Attempts to collect debt not owed   \n",
       "1            319XX              3229299     Written notification about debt   \n",
       "2            77069              3199379  Other features, terms, or problems   \n",
       "3            48066              2673060      Trouble during payment process   \n",
       "4            10473              3203545                    Fees or interest   \n",
       "\n",
       "       _source.date_received _source.state  ... _source.company_response  \\\n",
       "0  2019-04-13T12:00:00-05:00            CA  ...  Closed with explanation   \n",
       "1  2019-05-01T12:00:00-05:00            GA  ...  Closed with explanation   \n",
       "2  2019-04-02T12:00:00-05:00            TX  ...  Closed with explanation   \n",
       "3  2017-09-13T12:00:00-05:00            MI  ...  Closed with explanation   \n",
       "4  2019-04-05T12:00:00-05:00            NY  ...  Closed with explanation   \n",
       "\n",
       "        _source.company _source.submitted_via _source.date_sent_to_company  \\\n",
       "0  JPMORGAN CHASE & CO.                   Web    2019-04-13T12:00:00-05:00   \n",
       "1  JPMORGAN CHASE & CO.                   Web    2019-05-01T12:00:00-05:00   \n",
       "2  JPMORGAN CHASE & CO.                   Web    2019-04-02T12:00:00-05:00   \n",
       "3  JPMORGAN CHASE & CO.                   Web    2017-09-14T12:00:00-05:00   \n",
       "4  JPMORGAN CHASE & CO.              Referral    2019-04-05T12:00:00-05:00   \n",
       "\n",
       "  _source.company_public_response                         _source.sub_product  \\\n",
       "0                            None                            Credit card debt   \n",
       "1                            None                            Credit card debt   \n",
       "2                            None  General-purpose credit card or charge card   \n",
       "3                            None                  Conventional home mortgage   \n",
       "4                            None  General-purpose credit card or charge card   \n",
       "\n",
       "  _source.timely                    _source.complaint_what_happened  \\\n",
       "0            Yes                                                      \n",
       "1            Yes  Good morning my name is XXXX XXXX and I apprec...   \n",
       "2            Yes  I upgraded my XXXX XXXX card in XX/XX/2018 and...   \n",
       "3            Yes                                                      \n",
       "4            Yes                                                      \n",
       "\n",
       "                                  _source.sub_issue  \\\n",
       "0                                 Debt is not yours   \n",
       "1  Didn't receive enough information to verify debt   \n",
       "2             Problem with rewards from credit card   \n",
       "3                                              None   \n",
       "4                         Charged too much interest   \n",
       "\n",
       "  _source.consumer_consent_provided  \n",
       "0              Consent not provided  \n",
       "1                  Consent provided  \n",
       "2                  Consent provided  \n",
       "3              Consent not provided  \n",
       "4                               N/A  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the dataframe to understand the given data.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T02:54:20.179938Z",
     "iopub.status.busy": "2022-03-23T02:54:20.179600Z",
     "iopub.status.idle": "2022-03-23T02:54:20.186043Z",
     "shell.execute_reply": "2022-03-23T02:54:20.185403Z",
     "shell.execute_reply.started": "2022-03-23T02:54:20.179903Z"
    },
    "id": "Dwcty-wmJrFw",
    "outputId": "9b42f3b3-8e13-42b3-87e5-7606975fd8b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_index', '_type', '_id', '_score', '_source.tags', '_source.zip_code',\n",
       "       '_source.complaint_id', '_source.issue', '_source.date_received',\n",
       "       '_source.state', '_source.consumer_disputed', '_source.product',\n",
       "       '_source.company_response', '_source.company', '_source.submitted_via',\n",
       "       '_source.date_sent_to_company', '_source.company_public_response',\n",
       "       '_source.sub_product', '_source.timely',\n",
       "       '_source.complaint_what_happened', '_source.sub_issue',\n",
       "       '_source.consumer_consent_provided'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T02:54:21.028200Z",
     "iopub.status.busy": "2022-03-23T02:54:21.027727Z",
     "iopub.status.idle": "2022-03-23T02:54:21.036767Z",
     "shell.execute_reply": "2022-03-23T02:54:21.035796Z",
     "shell.execute_reply.started": "2022-03-23T02:54:21.028148Z"
    },
    "id": "FYCtKXD1JrFw"
   },
   "outputs": [],
   "source": [
    "#Assign new column names\n",
    "df.rename(columns={'_source.complaint_what_happened':'complaints_what_happened', '_source.product':'tag'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T02:54:22.042460Z",
     "iopub.status.busy": "2022-03-23T02:54:22.042099Z",
     "iopub.status.idle": "2022-03-23T02:54:22.048532Z",
     "shell.execute_reply": "2022-03-23T02:54:22.047924Z",
     "shell.execute_reply.started": "2022-03-23T02:54:22.042416Z"
    },
    "id": "B5CDC0rNyKOB",
    "outputId": "7a59a04c-c016-4e24-8dbe-f876ae79631a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_index', '_type', '_id', '_score', '_source.tags', '_source.zip_code',\n",
       "       '_source.complaint_id', '_source.issue', '_source.date_received',\n",
       "       '_source.state', '_source.consumer_disputed', 'tag',\n",
       "       '_source.company_response', '_source.company', '_source.submitted_via',\n",
       "       '_source.date_sent_to_company', '_source.company_public_response',\n",
       "       '_source.sub_product', '_source.timely', 'complaints_what_happened',\n",
       "       '_source.sub_issue', '_source.consumer_consent_provided'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T02:54:22.665111Z",
     "iopub.status.busy": "2022-03-23T02:54:22.664330Z",
     "iopub.status.idle": "2022-03-23T02:54:22.675086Z",
     "shell.execute_reply": "2022-03-23T02:54:22.674439Z",
     "shell.execute_reply.started": "2022-03-23T02:54:22.665066Z"
    },
    "id": "SH0io5lfyKOC"
   },
   "outputs": [],
   "source": [
    "df = df[['complaints_what_happened', 'tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T02:54:25.206560Z",
     "iopub.status.busy": "2022-03-23T02:54:25.206208Z",
     "iopub.status.idle": "2022-03-23T02:54:25.232474Z",
     "shell.execute_reply": "2022-03-23T02:54:25.231632Z",
     "shell.execute_reply.started": "2022-03-23T02:54:25.206525Z"
    },
    "id": "pWOMmADq_Jbe",
    "outputId": "9fb2c85f-47d1-40c4-e968-77f2c1f0f7e8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complaints_what_happened</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good morning my name is XXXX XXXX and I apprec...</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I upgraded my XXXX XXXX card in XX/XX/2018 and...</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chase Card was reported on XX/XX/2019. However...</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>On XX/XX/2018, while trying to book a XXXX  XX...</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>my grand son give me check for {$1600.00} i de...</td>\n",
       "      <td>Checking or savings account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78303</th>\n",
       "      <td>After being a Chase Card customer for well ove...</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78309</th>\n",
       "      <td>On Wednesday, XX/XX/XXXX I called Chas, my XXX...</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78310</th>\n",
       "      <td>I am not familiar with XXXX pay and did not un...</td>\n",
       "      <td>Checking or savings account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78311</th>\n",
       "      <td>I have had flawless credit for 30 yrs. I've ha...</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78312</th>\n",
       "      <td>Roughly 10+ years ago I closed out my accounts...</td>\n",
       "      <td>Payday loan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21072 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                complaints_what_happened  \\\n",
       "1      Good morning my name is XXXX XXXX and I apprec...   \n",
       "2      I upgraded my XXXX XXXX card in XX/XX/2018 and...   \n",
       "10     Chase Card was reported on XX/XX/2019. However...   \n",
       "11     On XX/XX/2018, while trying to book a XXXX  XX...   \n",
       "14     my grand son give me check for {$1600.00} i de...   \n",
       "...                                                  ...   \n",
       "78303  After being a Chase Card customer for well ove...   \n",
       "78309  On Wednesday, XX/XX/XXXX I called Chas, my XXX...   \n",
       "78310  I am not familiar with XXXX pay and did not un...   \n",
       "78311  I have had flawless credit for 30 yrs. I've ha...   \n",
       "78312  Roughly 10+ years ago I closed out my accounts...   \n",
       "\n",
       "                                                     tag  \n",
       "1                                        Debt collection  \n",
       "2                            Credit card or prepaid card  \n",
       "10     Credit reporting, credit repair services, or o...  \n",
       "11     Credit reporting, credit repair services, or o...  \n",
       "14                           Checking or savings account  \n",
       "...                                                  ...  \n",
       "78303                        Credit card or prepaid card  \n",
       "78309                        Credit card or prepaid card  \n",
       "78310                        Checking or savings account  \n",
       "78311                        Credit card or prepaid card  \n",
       "78312                                        Payday loan  \n",
       "\n",
       "[21072 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['complaints_what_happened'].astype(bool)]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L944HZpsJrFy"
   },
   "source": [
    "## Prepare the text for topic modeling\n",
    "\n",
    "After removing blank complaints:\n",
    "\n",
    "* Make the text lowercase\n",
    "* Remove text in square brackets\n",
    "* Remove punctuation\n",
    "* Remove words containing numbers\n",
    "\n",
    "\n",
    "After all the cleaning operations, performed the following:\n",
    "* Lemmatize the texts\n",
    "* Extract the POS tags of the lemmatized text and remove all the words which have tags other than NN[tag == \"NN\"].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T02:54:26.877709Z",
     "iopub.status.busy": "2022-03-23T02:54:26.876959Z",
     "iopub.status.idle": "2022-03-23T02:54:26.884225Z",
     "shell.execute_reply": "2022-03-23T02:54:26.883444Z",
     "shell.execute_reply.started": "2022-03-23T02:54:26.877663Z"
    },
    "id": "qm7SjjSkJrFz"
   },
   "outputs": [],
   "source": [
    "# Function to clean the text and remove all the unnecessary elements.\n",
    "def clean_data(text):\n",
    "    text = text.lower() # text to lowercase\n",
    "    text = re.sub(r'\\s\\{\\$\\S*', '',text) # Remove text within curly braces\n",
    "    text = re.sub(r'\\n', '', text) # Remove line breaks\n",
    "    text = re.sub(r'\\(\\w*\\)', '', text) #remove text within braces\n",
    "    text = re.sub(r'(\\W\\s)|(\\W$)|(\\W\\d*)', ' ',text) # Remove punctuation\n",
    "    text = re.sub(r'x+((/xx)*/\\d*\\s*)|x*', '',text) #Remove date\n",
    "    text = re.sub(r'\\d+\\s', '', text) #Remove other numerical values\n",
    "    text = re.sub(r' +', ' ',text) #Remove unnecessary white spaces\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T02:54:55.056274Z",
     "iopub.status.busy": "2022-03-23T02:54:55.055978Z",
     "iopub.status.idle": "2022-03-23T02:55:19.901791Z",
     "shell.execute_reply": "2022-03-23T02:55:19.900922Z",
     "shell.execute_reply.started": "2022-03-23T02:54:55.056240Z"
    },
    "id": "0uz1PjjDyKOD",
    "outputId": "7fc7da83-e481-464f-8115-b5334406d331"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complaints_what_happened</th>\n",
       "      <th>tag</th>\n",
       "      <th>complaints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good morning my name is XXXX XXXX and I apprec...</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>good morning my name is and i appreciate it if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I upgraded my XXXX XXXX card in XX/XX/2018 and...</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "      <td>i upgraded my card in and was told by the agen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chase Card was reported on XX/XX/2019. However...</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>chase card was reported on however fraudulent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>On XX/XX/2018, while trying to book a XXXX  XX...</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>on while trying to book a ticket i came across...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>my grand son give me check for {$1600.00} i de...</td>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>my grand son give me check for i deposit it in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             complaints_what_happened  \\\n",
       "1   Good morning my name is XXXX XXXX and I apprec...   \n",
       "2   I upgraded my XXXX XXXX card in XX/XX/2018 and...   \n",
       "10  Chase Card was reported on XX/XX/2019. However...   \n",
       "11  On XX/XX/2018, while trying to book a XXXX  XX...   \n",
       "14  my grand son give me check for {$1600.00} i de...   \n",
       "\n",
       "                                                  tag  \\\n",
       "1                                     Debt collection   \n",
       "2                         Credit card or prepaid card   \n",
       "10  Credit reporting, credit repair services, or o...   \n",
       "11  Credit reporting, credit repair services, or o...   \n",
       "14                        Checking or savings account   \n",
       "\n",
       "                                           complaints  \n",
       "1   good morning my name is and i appreciate it if...  \n",
       "2   i upgraded my card in and was told by the agen...  \n",
       "10  chase card was reported on however fraudulent ...  \n",
       "11  on while trying to book a ticket i came across...  \n",
       "14  my grand son give me check for i deposit it in...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply data cleaning to the complaints_what_happened column\n",
    "\n",
    "df['complaints'] = df['complaints_what_happened'].apply(clean_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T02:55:19.904599Z",
     "iopub.status.busy": "2022-03-23T02:55:19.903733Z",
     "iopub.status.idle": "2022-03-23T02:55:19.910778Z",
     "shell.execute_reply": "2022-03-23T02:55:19.909972Z",
     "shell.execute_reply.started": "2022-03-23T02:55:19.904549Z"
    },
    "id": "zgOu8t8HJrFz"
   },
   "outputs": [],
   "source": [
    "# Function to Lemmatize the texts\n",
    "def lemmatization(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    wordnet_lemmetizer = WordNetLemmatizer()\n",
    "    lemmatized = [wordnet_lemmetizer.lemmatize(token) for token in tokens]\n",
    "    lemmatized_str = \" \".join(lemmatized)\n",
    "    return lemmatized_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T02:55:20.113397Z",
     "iopub.status.busy": "2022-03-23T02:55:20.113158Z",
     "iopub.status.idle": "2022-03-23T02:56:27.818951Z",
     "shell.execute_reply": "2022-03-23T02:56:27.817920Z",
     "shell.execute_reply.started": "2022-03-23T02:55:20.113345Z"
    },
    "id": "uXnN7aa_JrF0"
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/amanchourasiya/nltk_data'\n    - '/Users/amanchourasiya/opt/anaconda3/nltk_data'\n    - '/Users/amanchourasiya/opt/anaconda3/share/nltk_data'\n    - '/Users/amanchourasiya/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ds/6hyhnc9579309ff5gz8k_c1h0000gn/T/ipykernel_16266/3433699321.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Create a dataframe('df_clean') that will have only the complaints and the lemmatized complaints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'complaints'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'complaints'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lemmatized'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'complaints'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4431\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4432\u001b[0m         \"\"\"\n\u001b[0;32m-> 4433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4435\u001b[0m     def _reduce(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1144\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/var/folders/ds/6hyhnc9579309ff5gz8k_c1h0000gn/T/ipykernel_16266/4015749098.py\u001b[0m in \u001b[0;36mlemmatization\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Function to Lemmatize the texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mwordnet_lemmetizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlemmatized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwordnet_lemmetizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/amanchourasiya/nltk_data'\n    - '/Users/amanchourasiya/opt/anaconda3/nltk_data'\n    - '/Users/amanchourasiya/opt/anaconda3/share/nltk_data'\n    - '/Users/amanchourasiya/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#Create a dataframe('df_clean') that will have only the complaints and the lemmatized complaints \n",
    "df_clean = pd.DataFrame({'complaints':df['complaints'], 'lemmatized':df['complaints'].apply(lemmatization)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T02:56:27.822309Z",
     "iopub.status.busy": "2022-03-23T02:56:27.821994Z",
     "iopub.status.idle": "2022-03-23T02:56:27.832190Z",
     "shell.execute_reply": "2022-03-23T02:56:27.831321Z",
     "shell.execute_reply.started": "2022-03-23T02:56:27.822268Z"
    },
    "id": "nOiDVvEIJrF0",
    "outputId": "93738e15-e419-4c65-b91c-aaaeed35b3cd"
   },
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T02:56:27.833582Z",
     "iopub.status.busy": "2022-03-23T02:56:27.833339Z",
     "iopub.status.idle": "2022-03-23T03:04:39.903922Z",
     "shell.execute_reply": "2022-03-23T03:04:39.902756Z",
     "shell.execute_reply.started": "2022-03-23T02:56:27.833555Z"
    },
    "id": "Kk7fc4DuJrF1"
   },
   "outputs": [],
   "source": [
    "#Write your function to extract the POS tags \n",
    "\n",
    "def get_pos_tags(text):\n",
    "    nn_words = []\n",
    "    doc = nlp(text)\n",
    "    for tok in doc:\n",
    "        if(tok.tag_ == 'NN'):\n",
    "            nn_words.append(tok.lemma_)\n",
    "    nn_words_str = \" \".join(nn_words)\n",
    "    return nn_words_str\n",
    "\n",
    "#this column should contain lemmatized text with all the words removed which have tags other than NN[tag == \"NN\"].\n",
    "df_clean[\"complaint_POS_removed\"] =  df_clean.swifter.apply(lambda x: get_pos_tags(x['lemmatized']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:04:39.906273Z",
     "iopub.status.busy": "2022-03-23T03:04:39.905980Z",
     "iopub.status.idle": "2022-03-23T03:04:39.923621Z",
     "shell.execute_reply": "2022-03-23T03:04:39.922970Z",
     "shell.execute_reply.started": "2022-03-23T03:04:39.906236Z"
    },
    "id": "AjxfchvFJrF2",
    "outputId": "5a6966a4-d02a-4110-ea2d-73d6ff42176a"
   },
   "outputs": [],
   "source": [
    "#The clean dataframe should now contain the raw complaint, lemmatized complaint and the complaint after removing POS tags.\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7Un1AElJrF2"
   },
   "source": [
    "## Exploratory data analysis to get familiar with the data.\n",
    "\n",
    "Data visulaisation:\n",
    "\n",
    "*   Visualise the data according to the 'Complaint' character length\n",
    "*   Using a word cloud find the top 40 words by frequency among all the articles after processing the text\n",
    "*   Find the top unigrams,bigrams and trigrams by frequency among all the complaints after processing the text. â\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:04:39.925016Z",
     "iopub.status.busy": "2022-03-23T03:04:39.924803Z",
     "iopub.status.idle": "2022-03-23T03:04:40.532414Z",
     "shell.execute_reply": "2022-03-23T03:04:40.531543Z",
     "shell.execute_reply.started": "2022-03-23T03:04:39.924990Z"
    },
    "id": "q-zaqJF6JrF2",
    "outputId": "6b18692c-95f2-462e-92a8-e1246012b7a0"
   },
   "outputs": [],
   "source": [
    "# Write your code here to visualise the data according to the 'Complaint' character length\n",
    "plt.figure(figsize=(10,6))\n",
    "doc_lens = [len(d) for d in df_clean.complaint_POS_removed]\n",
    "plt.hist(doc_lens, bins = 100)\n",
    "plt.ylabel('Number of Complaint')\n",
    "plt.xlabel('Complaint character length')\n",
    "sns.despine();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9jD_6SeJrF3"
   },
   "source": [
    "#### Find the top 40 words by frequency among all the articles after processing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:04:40.534973Z",
     "iopub.status.busy": "2022-03-23T03:04:40.533987Z",
     "iopub.status.idle": "2022-03-23T03:04:40.960454Z",
     "shell.execute_reply": "2022-03-23T03:04:40.959735Z",
     "shell.execute_reply.started": "2022-03-23T03:04:40.534927Z"
    },
    "id": "VzDvFyzrFE2U",
    "outputId": "1115fe65-2d86-4738-8e07-ce3697c89c14"
   },
   "outputs": [],
   "source": [
    "# Top 40 words frequency wise wordcloud\n",
    "wordcloud = WordCloud(max_words=40, random_state=1, stopwords=set(STOPWORDS))\n",
    "wordcloud.generate(str(df_clean['complaint_POS_removed']))\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:04:40.964017Z",
     "iopub.status.busy": "2022-03-23T03:04:40.963291Z",
     "iopub.status.idle": "2022-03-23T03:04:41.008025Z",
     "shell.execute_reply": "2022-03-23T03:04:41.007400Z",
     "shell.execute_reply.started": "2022-03-23T03:04:40.963939Z"
    },
    "id": "OkSmc3UaJrF4"
   },
   "outputs": [],
   "source": [
    "#Removing -PRON- from the text corpus\n",
    "df_clean['Complaint_clean'] = df_clean['complaint_POS_removed'].str.replace('-PRON-', '')\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DfCSbbmJrF4"
   },
   "source": [
    "#### Find the top unigrams,bigrams and trigrams by frequency among all the complaints after processing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:04:41.010021Z",
     "iopub.status.busy": "2022-03-23T03:04:41.009250Z",
     "iopub.status.idle": "2022-03-23T03:04:41.016675Z",
     "shell.execute_reply": "2022-03-23T03:04:41.015893Z",
     "shell.execute_reply.started": "2022-03-23T03:04:41.009979Z"
    }
   },
   "outputs": [],
   "source": [
    "def top_grams(grams):\n",
    "    c_vec = CountVectorizer(stop_words=stopwords.words('english'), ngram_range=(grams,grams))\n",
    "    grams = c_vec.fit_transform(df_clean['complaints'])\n",
    "    count_values = grams.toarray().sum(axis=0)\n",
    "    vocab = c_vec.vocabulary_\n",
    "    df_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n",
    "            ).rename(columns={0: 'frequency', 1:'unigram'})\n",
    "    return df_ngram\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:04:41.018085Z",
     "iopub.status.busy": "2022-03-23T03:04:41.017752Z",
     "iopub.status.idle": "2022-03-23T03:04:49.254449Z",
     "shell.execute_reply": "2022-03-23T03:04:49.253607Z",
     "shell.execute_reply.started": "2022-03-23T03:04:41.018056Z"
    }
   },
   "outputs": [],
   "source": [
    "df_unigram = top_grams(1)\n",
    "df_unigram.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:04:49.256515Z",
     "iopub.status.busy": "2022-03-23T03:04:49.256022Z",
     "iopub.status.idle": "2022-03-23T03:05:58.724242Z",
     "shell.execute_reply": "2022-03-23T03:05:58.723428Z",
     "shell.execute_reply.started": "2022-03-23T03:04:49.256470Z"
    }
   },
   "outputs": [],
   "source": [
    "df_bigram = top_grams(2)\n",
    "df_bigram.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:05:58.726087Z",
     "iopub.status.busy": "2022-03-23T03:05:58.725778Z",
     "iopub.status.idle": "2022-03-23T03:08:27.934430Z",
     "shell.execute_reply": "2022-03-23T03:08:27.933447Z",
     "shell.execute_reply.started": "2022-03-23T03:05:58.726045Z"
    }
   },
   "outputs": [],
   "source": [
    "df_trigram = top_grams(3)\n",
    "df_trigram.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:27.936979Z",
     "iopub.status.busy": "2022-03-23T03:08:27.936404Z",
     "iopub.status.idle": "2022-03-23T03:08:27.953801Z",
     "shell.execute_reply": "2022-03-23T03:08:27.952900Z",
     "shell.execute_reply.started": "2022-03-23T03:08:27.936933Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-I0k0QtJrGA"
   },
   "source": [
    "## Feature Extraction\n",
    "Convert the raw texts to a matrix of TF-IDF features\n",
    "\n",
    "**max_df** is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\"\n",
    "max_df = 0.95 means \"ignore terms that appear in more than 95% of the complaints\"\n",
    "\n",
    "**min_df** is used for removing terms that appear too infrequently\n",
    "min_df = 2 means \"ignore terms that appear in less than 2 complaints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:27.955902Z",
     "iopub.status.busy": "2022-03-23T03:08:27.955047Z",
     "iopub.status.idle": "2022-03-23T03:08:27.966275Z",
     "shell.execute_reply": "2022-03-23T03:08:27.965454Z",
     "shell.execute_reply.started": "2022-03-23T03:08:27.955851Z"
    },
    "id": "Y8fGwaCPJrGA"
   },
   "outputs": [],
   "source": [
    "#Write your code here to initialise the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYzD85nTJrGA"
   },
   "source": [
    "#### Create a document term matrix using fit_transform\n",
    "\n",
    "The contents of a document term matrix are tuples of (complaint_id,token_id) tf-idf score:\n",
    "The tuples that are not there have a tf-idf score of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:27.968157Z",
     "iopub.status.busy": "2022-03-23T03:08:27.967860Z",
     "iopub.status.idle": "2022-03-23T03:08:32.233180Z",
     "shell.execute_reply": "2022-03-23T03:08:32.232339Z",
     "shell.execute_reply.started": "2022-03-23T03:08:27.968125Z"
    },
    "id": "ffzdDpp_JrGB"
   },
   "outputs": [],
   "source": [
    "#Write your code here to create the Document Term Matrix by transforming the complaints column present in df_clean.\n",
    "dtm = tfidf.fit_transform(df_clean['complaints'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Q9lwvNEJrGB"
   },
   "source": [
    "## Topic Modelling using NMF\n",
    "\n",
    "Non-Negative Matrix Factorization (NMF) is an unsupervised technique so there are no labeling of topics that the model will be trained on. The way it works is that, NMF decomposes (or factorizes) high-dimensional vectors into a lower-dimensional representation. These lower-dimensional vectors are non-negative which also means their coefficients are non-negative.\n",
    "\n",
    "In this task we will perform the following:\n",
    "\n",
    "* Find the best number of clusters \n",
    "* Apply the best number to create word clusters\n",
    "* Inspect & validate the correction of each cluster wrt the complaints \n",
    "* Correct the labels if needed \n",
    "* Map the clusters to topics/cluster names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:32.234690Z",
     "iopub.status.busy": "2022-03-23T03:08:32.234432Z",
     "iopub.status.idle": "2022-03-23T03:08:32.264807Z",
     "shell.execute_reply": "2022-03-23T03:08:32.263998Z",
     "shell.execute_reply.started": "2022-03-23T03:08:32.234660Z"
    },
    "id": "amLT4omWJrGB"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wYR1xUTJrGD"
   },
   "source": [
    "## Manual Topic Modeling\n",
    "We need to do take the trial & error approach to find the best num of topics for your NMF model.\n",
    "\n",
    "The only parameter that is required is the number of components i.e. the number of topics we want. This is the most crucial step in the whole topic modeling process and will greatly affect how good our final topics are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:32.266809Z",
     "iopub.status.busy": "2022-03-23T03:08:32.265988Z",
     "iopub.status.idle": "2022-03-23T03:08:32.270144Z",
     "shell.execute_reply": "2022-03-23T03:08:32.269609Z",
     "shell.execute_reply.started": "2022-03-23T03:08:32.266765Z"
    },
    "id": "sgd2A6bhJrGD"
   },
   "outputs": [],
   "source": [
    "#Load your nmf_model with the n_components i.e 5\n",
    "num_topics = 5\n",
    "\n",
    "#keep the random_state =40\n",
    "nmf_model = NMF(random_state=40, n_components=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:32.271327Z",
     "iopub.status.busy": "2022-03-23T03:08:32.271035Z",
     "iopub.status.idle": "2022-03-23T03:08:35.462220Z",
     "shell.execute_reply": "2022-03-23T03:08:35.461045Z",
     "shell.execute_reply.started": "2022-03-23T03:08:32.271301Z"
    },
    "id": "VPMDYbt_JrGE"
   },
   "outputs": [],
   "source": [
    "nmf_model.fit(dtm)\n",
    "len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:35.464625Z",
     "iopub.status.busy": "2022-03-23T03:08:35.463926Z",
     "iopub.status.idle": "2022-03-23T03:08:35.541617Z",
     "shell.execute_reply": "2022-03-23T03:08:35.540775Z",
     "shell.execute_reply.started": "2022-03-23T03:08:35.464576Z"
    },
    "id": "16kRfat5JrGE"
   },
   "outputs": [],
   "source": [
    "#Print the Top15 words for each of the topics\n",
    "words = np.array(tfidf.get_feature_names())\n",
    "topic_words_df = pd.DataFrame(np.zeros((num_topics, 15)), index=[f'Topic {i + 1}' for i in range(num_topics)],\n",
    "                           columns=[f'Word {i + 1}' for i in range(15)]).astype(str)\n",
    "\n",
    "for i in range(num_topics):\n",
    "    ix = nmf_model.components_[i].argsort()[::-1][:15]\n",
    "    topic_words_df.iloc[i] = words[ix]\n",
    "\n",
    "topic_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:35.543920Z",
     "iopub.status.busy": "2022-03-23T03:08:35.543352Z",
     "iopub.status.idle": "2022-03-23T03:08:35.617509Z",
     "shell.execute_reply": "2022-03-23T03:08:35.616806Z",
     "shell.execute_reply.started": "2022-03-23T03:08:35.543875Z"
    },
    "id": "0OIT7LmFJrGF"
   },
   "outputs": [],
   "source": [
    "#Create the best topic for each complaint in terms of integer value 0,1,2,3 & 4\n",
    "topic_results = nmf_model.transform(dtm)\n",
    "topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:35.619239Z",
     "iopub.status.busy": "2022-03-23T03:08:35.618813Z",
     "iopub.status.idle": "2022-03-23T03:08:35.624051Z",
     "shell.execute_reply": "2022-03-23T03:08:35.623437Z",
     "shell.execute_reply.started": "2022-03-23T03:08:35.619209Z"
    },
    "id": "peyYv-ORJrGF"
   },
   "outputs": [],
   "source": [
    "#Assign the best topic to each of the cmplaints in Topic Column\n",
    "\n",
    "df_clean['Topic'] = topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:35.625685Z",
     "iopub.status.busy": "2022-03-23T03:08:35.625284Z",
     "iopub.status.idle": "2022-03-23T03:08:35.643422Z",
     "shell.execute_reply": "2022-03-23T03:08:35.642849Z",
     "shell.execute_reply.started": "2022-03-23T03:08:35.625637Z"
    },
    "id": "fLh_Gf3nJrGF"
   },
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:35.645096Z",
     "iopub.status.busy": "2022-03-23T03:08:35.644690Z",
     "iopub.status.idle": "2022-03-23T03:08:35.671967Z",
     "shell.execute_reply": "2022-03-23T03:08:35.671135Z",
     "shell.execute_reply.started": "2022-03-23T03:08:35.645024Z"
    },
    "id": "aQKpufSPJrGG"
   },
   "outputs": [],
   "source": [
    "# Print the first 5 Complaint for each of the Topics\n",
    "First5_comp=df_clean.groupby('Topic').head(5)\n",
    "First5_comp.sort_values('Topic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piyLxzj6v07j"
   },
   "source": [
    "#### After evaluating the mapping, if the topics assigned are correct then assign these names to the relevant topic:\n",
    "* Bank Account services\n",
    "* Credit card or prepaid card\n",
    "* Theft/Dispute Reporting\n",
    "* Mortgage/Loan\n",
    "* Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:35.673872Z",
     "iopub.status.busy": "2022-03-23T03:08:35.673474Z",
     "iopub.status.idle": "2022-03-23T03:08:35.682153Z",
     "shell.execute_reply": "2022-03-23T03:08:35.680952Z",
     "shell.execute_reply.started": "2022-03-23T03:08:35.673824Z"
    },
    "id": "TWpwDG4RJrGG"
   },
   "outputs": [],
   "source": [
    "# Create the dictionary of Topic names and Topics\n",
    "Topic_names = {0:'Account Services', 1:'Others', 2:'Mortgage/Loan', 3:'Credit card or prepaid card', 4:'Theft/Dispute Reporting'}\n",
    "\n",
    "# Replace Topics with Topic Names\n",
    "df_clean['Topic'] = df_clean['Topic'].map(Topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:35.684505Z",
     "iopub.status.busy": "2022-03-23T03:08:35.683735Z",
     "iopub.status.idle": "2022-03-23T03:08:35.702860Z",
     "shell.execute_reply": "2022-03-23T03:08:35.702067Z",
     "shell.execute_reply.started": "2022-03-23T03:08:35.684459Z"
    },
    "id": "-2ULY5K6JrGG"
   },
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Mu0QBOcJrGH"
   },
   "source": [
    "## Supervised model to predict any new complaints to the relevant Topics.\n",
    "\n",
    "We have built a model to create the topics for each complaints.Now in the below section we will use them to classify any new complaints.\n",
    "\n",
    "*Since we will be using supervised learning technique we have to convert the topic names to numbers(numpy arrays only understand numbers)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:35.707445Z",
     "iopub.status.busy": "2022-03-23T03:08:35.707198Z",
     "iopub.status.idle": "2022-03-23T03:08:35.715831Z",
     "shell.execute_reply": "2022-03-23T03:08:35.715231Z",
     "shell.execute_reply.started": "2022-03-23T03:08:35.707414Z"
    },
    "id": "_U8J3J8wJrGH"
   },
   "outputs": [],
   "source": [
    "# Create the dictionary again of Topic names and Topics\n",
    "\n",
    "Topic_names = {'Account Services':0, 'Others':1, 'Mortgage/Loan':2, 'Credit card or prepaid card':3, 'Theft/Dispute Reporting':4}\n",
    "# Replace Topics with Topic Names\n",
    "df_clean['Topic'] = df_clean['Topic'].map(Topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:35.717271Z",
     "iopub.status.busy": "2022-03-23T03:08:35.717055Z",
     "iopub.status.idle": "2022-03-23T03:08:35.740571Z",
     "shell.execute_reply": "2022-03-23T03:08:35.739397Z",
     "shell.execute_reply.started": "2022-03-23T03:08:35.717245Z"
    },
    "id": "BWIgJUkQJrGH"
   },
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:35.742613Z",
     "iopub.status.busy": "2022-03-23T03:08:35.741710Z",
     "iopub.status.idle": "2022-03-23T03:08:35.748933Z",
     "shell.execute_reply": "2022-03-23T03:08:35.748397Z",
     "shell.execute_reply.started": "2022-03-23T03:08:35.742572Z"
    },
    "id": "Xx-FrbkWJrGH"
   },
   "outputs": [],
   "source": [
    "# Keep the columns\"complaint_what_happened\" & \"Topic\" only in the new dataframe --> training_data\n",
    "training_data = df_clean.drop(['lemmatized', 'complaint_POS_removed', 'Complaint_clean'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:35.750165Z",
     "iopub.status.busy": "2022-03-23T03:08:35.749921Z",
     "iopub.status.idle": "2022-03-23T03:08:35.768170Z",
     "shell.execute_reply": "2022-03-23T03:08:35.767339Z",
     "shell.execute_reply.started": "2022-03-23T03:08:35.750137Z"
    },
    "id": "lVg2pa12JrGI"
   },
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "280Vbqk-7a8M"
   },
   "source": [
    "#### Applying the supervised models on the training data created. In this process, we are going to do the following:\n",
    "* Create the vector counts using Count Vectoriser\n",
    "* Transform the word vecotr to tf-idf\n",
    "* Create the train & test data using the train_test_split on the tf-idf & topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:35.769730Z",
     "iopub.status.busy": "2022-03-23T03:08:35.769419Z",
     "iopub.status.idle": "2022-03-23T03:08:40.779654Z",
     "shell.execute_reply": "2022-03-23T03:08:40.778769Z",
     "shell.execute_reply.started": "2022-03-23T03:08:35.769696Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code to get the Vector count\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(training_data.complaints)\n",
    "\n",
    "# Write your code here to transform the word vector to tf-idf\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMU3vj6w-wqL"
   },
   "source": [
    "You have to try atleast 3 models on the train & test data from these options:\n",
    "* Logistic regression\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "* Naive Bayes (optional)\n",
    "\n",
    "**Using the required evaluation metrics judge the tried models and select the ones performing the best**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:40.782198Z",
     "iopub.status.busy": "2022-03-23T03:08:40.781183Z",
     "iopub.status.idle": "2022-03-23T03:08:40.933622Z",
     "shell.execute_reply": "2022-03-23T03:08:40.932680Z",
     "shell.execute_reply.started": "2022-03-23T03:08:40.782146Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:40.937671Z",
     "iopub.status.busy": "2022-03-23T03:08:40.937347Z",
     "iopub.status.idle": "2022-03-23T03:08:40.958845Z",
     "shell.execute_reply": "2022-03-23T03:08:40.957663Z",
     "shell.execute_reply.started": "2022-03-23T03:08:40.937634Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data.Topic, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:40.961779Z",
     "iopub.status.busy": "2022-03-23T03:08:40.960915Z",
     "iopub.status.idle": "2022-03-23T03:08:53.952849Z",
     "shell.execute_reply": "2022-03-23T03:08:53.951800Z",
     "shell.execute_reply.started": "2022-03-23T03:08:40.961727Z"
    },
    "id": "udLHpPsZJrGI"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "predicted = lr.predict(X_test)\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:08:53.962057Z",
     "iopub.status.busy": "2022-03-23T03:08:53.958800Z",
     "iopub.status.idle": "2022-03-23T03:09:09.664596Z",
     "shell.execute_reply": "2022-03-23T03:09:09.663653Z",
     "shell.execute_reply.started": "2022-03-23T03:08:53.961984Z"
    },
    "id": "N2OznsObJrGP"
   },
   "outputs": [],
   "source": [
    "# Decision tree classifier\n",
    "dt = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "predicted = dt.predict(X_test)\n",
    "\n",
    "print(classification_report(y_pred=predicted, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:09:09.666770Z",
     "iopub.status.busy": "2022-03-23T03:09:09.666423Z",
     "iopub.status.idle": "2022-03-23T03:09:14.160210Z",
     "shell.execute_reply": "2022-03-23T03:09:14.159145Z",
     "shell.execute_reply.started": "2022-03-23T03:09:09.666727Z"
    }
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth=10)\n",
    "rfc.fit(X_train, y_train)\n",
    "predicted = rfc.predict(X_test)\n",
    "\n",
    "print(classification_report(y_pred=predicted, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Gaussian Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:09:14.162704Z",
     "iopub.status.busy": "2022-03-23T03:09:14.161932Z",
     "iopub.status.idle": "2022-03-23T03:09:29.108123Z",
     "shell.execute_reply": "2022-03-23T03:09:29.107166Z",
     "shell.execute_reply.started": "2022-03-23T03:09:14.162653Z"
    }
   },
   "outputs": [],
   "source": [
    "nb = GaussianNB().fit(X_train.toarray(), y_train)\n",
    "predicted = nb.predict(X_test.toarray())\n",
    "\n",
    "print(classification_report(y_pred=predicted, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clearly Logistic Regression is performing better**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infering the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:09:29.109771Z",
     "iopub.status.busy": "2022-03-23T03:09:29.109460Z",
     "iopub.status.idle": "2022-03-23T03:09:29.120407Z",
     "shell.execute_reply": "2022-03-23T03:09:29.119539Z",
     "shell.execute_reply.started": "2022-03-23T03:09:29.109736Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some sample complaints to infer model\n",
    "\n",
    "df_complaints = pd.DataFrame({'complaints': [\"I can not get from chase who services my mortgage, who owns it and who has original loan docs\", \n",
    "                                  \"The bill amount of my credit card was debited twice. Please look into the matter and resolve at the earliest.\",\n",
    "                                  \"I want to open a salary account at your downtown branch. Please provide me the procedure.\",\n",
    "                                  \"Yesterday, I received a fraudulent email regarding renewal of my services.\",\n",
    "                                  \"What is the procedure to know my CIBIL score?\",\n",
    "                                  \"I need to know the number of bank branches and their locations in the city of Dubai\"]})\n",
    "df_complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:09:29.150733Z",
     "iopub.status.busy": "2022-03-23T03:09:29.150305Z",
     "iopub.status.idle": "2022-03-23T03:09:29.159783Z",
     "shell.execute_reply": "2022-03-23T03:09:29.158792Z",
     "shell.execute_reply.started": "2022-03-23T03:09:29.150699Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_lr(text):\n",
    "    Topic_names = {0:'Account Services', 1:'Others', 2:'Mortgage/Loan', 3:'Credit card or prepaid card', 4:'Theft/Dispute Reporting'}\n",
    "    X_new_counts = count_vect.transform(text)\n",
    "    X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "    predicted = lr.predict(X_new_tfidf)\n",
    "    return Topic_names[predicted[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T03:19:44.166691Z",
     "iopub.status.busy": "2022-03-23T03:19:44.166315Z",
     "iopub.status.idle": "2022-03-23T03:19:44.189879Z",
     "shell.execute_reply": "2022-03-23T03:19:44.189316Z",
     "shell.execute_reply.started": "2022-03-23T03:19:44.166656Z"
    }
   },
   "outputs": [],
   "source": [
    "df_complaints['tag'] = df_complaints['complaints'].apply(lambda x: predict_lr([x]))\n",
    "df_complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "    \n",
    "* **As expected 5 topics were indetified namely:**\n",
    "    1. Account Services\n",
    "    2. Others\n",
    "    3. Mortgage/Loan\n",
    "    4. Credit card or prepaid card\n",
    "    5. Theft/Dispute Reporting\n",
    "    \n",
    "    \n",
    "* **Tried 4 models on the data with accuracies as follows:**\n",
    "\n",
    "    | Model       | Accuracy |\n",
    "    | ----------- | ----------- |\n",
    "    | Logistic Regression      | 0.95       |\n",
    "    | Decision Tree   | 0.77        |\n",
    "    | Random Forest      | 0.74       |\n",
    "    | Naive Bayes   | 0.36        |\n",
    "    \n",
    " <br /> <br /> <br /> <br /><br /> <br /><br />\n",
    "* **Logistic Regression has highest accuracy of `0.95`, Hence is a good fit for this particular case study.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
